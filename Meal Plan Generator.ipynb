{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://www.hellofresh.com/recipes/vegetarian-recipes?page=50\"\n",
    "#grabs list of vegetarian recipes from hellofresh, \n",
    "#unfortunately not a complete list of vegetarian recipes, but a large number nonetheless.\n",
    "page = requests.get(URL)\n",
    "#saves url as page\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "#saves page as BeautifulSoup object, parsing for html\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#URL = \"https://www.hellofresh.com/recipes/vegetarian-recipes?page=50\"\n",
    "#grabs list of vegetarian recipes from hellofresh, \n",
    "#unfortunately not a complete list of vegetarian recipes, but a large number nonetheless.\n",
    "f = open(\"vegetarian.html\") # generate vegetarian.html by going to above URL, letting cards load, open element inspector and copy outer HTML that includes all cards\n",
    "#saves url as page\n",
    "soup = BeautifulSoup(f, \"html.parser\")\n",
    "#saves page as BeautifulSoup object, parsing for html\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "title_elements = [title.get_text() for title in soup.find_all('h3')]\n",
    "#looks for the title text of all h3 classes\n",
    "titles_start = title_elements.index('Dishes')+1\n",
    "#create index where recipe titles start\n",
    "titles_end = title_elements.index('Recipes by Ingredient')\n",
    "#creates an index for where the recipe titles end\n",
    "titles = title_elements[titles_start:titles_end-0]\n",
    "#creates list of cleaned recipe titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_elements2 = [link.get('href') for link in soup.find_all('a')]\n",
    "#gets all the links from the web page under the 'a' class\n",
    "target_ibdex1 = text_elements2.index('/recipes/ingredients')\n",
    "#creates index for end of link list\n",
    "target_ibdex2 = text_elements2.index('/recipes/casserole-recipes')\n",
    "#creates index for beginning of link list\n",
    "links = text_elements2[target_ibdex2+1:target_ibdex1-0]\n",
    "#creates clean list of links that correspond to title list\n",
    "\n",
    "#for i in range( len(links)):\n",
    "#    links[i] = \"https://www.hellofresh.com\" + links[i]\n",
    "#adds the full url so it's a clickable link\n",
    "#delete above url prefix as links already contains them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import pandas as pd\n",
    "#importing pandas to create dataframes\n",
    "recipe_df = pd.DataFrame(list(zip(titles, links)),\n",
    "               columns =['Name', 'Link'])\n",
    "#creates dataframe with the recipe title and link to recipe\n",
    "recipe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_recipes = recipe_df.sample(n=2)\n",
    "#creates a sub-dataframe from the recipe dataframe above, selecting two recipes at random.\n",
    "#change n number to increase or decrease random recipe amounts. \n",
    "#(will have to add to the below code to compensate for changes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Recipe\n",
    "recipe_URL = random_recipes.iloc[0]['Link']\n",
    "#pulls string of first link\n",
    "recipe_page = requests.get(recipe_URL)\n",
    "#grabs the web page for first recipe\n",
    "recipe_soup = BeautifulSoup(recipe_page.content, \"html.parser\")\n",
    "#creates a soup object for easier html parsing\n",
    "\n",
    "#Second Recipe\n",
    "recipe_URL2 = random_recipes.iloc[1]['Link']\n",
    "#pulls string of second link\n",
    "recipe_page2 = requests.get(recipe_URL2)\n",
    "#grabs the web page for second recipe\n",
    "recipe_soup2 = BeautifulSoup(recipe_page2.content, \"html.parser\")\n",
    "#creates a soup object for easier html parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitelist = [\n",
    "  'p'\n",
    "]\n",
    "#Creates a whitelist of only 'p' html classes, which is where the recipe elements are.\n",
    "recipe_elements = [t for t in recipe_soup.find_all(text=True) if t.parent.name in whitelist]\n",
    "#Finds all text objects from 'p' html classes\n",
    "\n",
    "#target_ibdex = recipe_elements.index('Kosher Salt')\n",
    "#Final recipe item index\n",
    "#removed for safety; depends on salt being last ingredient\n",
    "\n",
    "target_ibdex = next(i for i,v in enumerate(recipe_elements) if i>0 and len(v)>100) #zeroth long line is description; first is instructions\n",
    "recipe_parts = recipe_elements[1:target_ibdex-0]\n",
    "#gets all the ingredients from the recipe starting from the second object to the created index\n",
    "recipe = [' '.join(recipe_parts[i:i+2]) for i in range(0, len(recipe_parts), 2)]\n",
    "#creates a list to join amounts and ingredient labels for easy viewing\n",
    "\n",
    "recipe_elements2 = [t for t in recipe_soup2.find_all(text=True) if t.parent.name in whitelist]\n",
    "#Finds all text objects from 'p' html classes\n",
    "target_ibdex3 = recipe_elements2.index('Kosher Salt')\n",
    "#Final recipe item index\n",
    "recipe_parts2 = recipe_elements2[1:target_ibdex3-0]\n",
    "#gets all the ingredients from the recipe starting from the second object to the created index\n",
    "recipe2 = [' '.join(recipe_parts2[i:i+2]) for i in range(0, len(recipe_parts2), 2)]\n",
    "#creates a list to join amounts and ingredient labels for easy viewing\n",
    "\n",
    "\n",
    "grocerylist_df = pd.DataFrame({\"ingredient\":recipe + recipe2})\n",
    "#creates dataframe of both recipe ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grocerylist_df_clean = grocerylist_df[grocerylist_df[\"ingredient\"].str.contains(\"unit\")==False]\n",
    "#cleans dataframe of non-ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random_recipes.iloc[0,0])\n",
    "#prints first recipe title\n",
    "print(random_recipes.iloc[1,0])\n",
    "#prints second recipe title\n",
    "print(grocerylist_df_clean)\n",
    "#prints clean grocery list\n",
    "print(random_recipes.iloc[0,1])\n",
    "#prints first recipe link (clickable)\n",
    "print(random_recipes.iloc[1,1])\n",
    "#prints second recipe link (clickable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
